{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rodrigozago/compiladores/blob/master/Lexico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh9K9qYJ9t23"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2vT3_Y0DbZR"
   },
   "source": [
    "Analisador\n",
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOwtiPdOC3mm"
   },
   "source": [
    "Definição da tabela de simbolos\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLc-ORg8omQA"
   },
   "source": [
    "Classe que define a tabela de simbolos.\n",
    "\n",
    "symTable: Estrutura do tipo dict para armazenar os tokens:\n",
    "\n",
    "```{ lexema<string> : token<Token> }```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "stf4cugLAO69"
   },
   "outputs": [],
   "source": [
    "class SymTable:\n",
    "    def __init__(self):\n",
    "        self.symTable = dict();\n",
    "    def addNewEntry(self, token):\n",
    "        if(token.lexema not in self.symTable):\n",
    "            self.symTable[token.lexema] = token\n",
    "        return token\n",
    "    def getEntry(self, lexema):\n",
    "        if(lexema in self.symTable):\n",
    "            return self.symTable[lexema]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuVejW89C-QD"
   },
   "source": [
    "Definição da classe Token\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9532H6Ww13I-"
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, lexema, token, tipo):\n",
    "        self.lexema = lexema\n",
    "        self.token = token\n",
    "        self.tipo = tipo\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Lexema: {} Token: {} Tipo: {}\".format(self.lexema, self.token, self.tipo)\n",
    "\n",
    "    def getList(self):\n",
    "        return [self.lexema, self.token, self.tipo]\n",
    "    \n",
    "    def get(self):\n",
    "        return self.token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hygyYiRGDLif"
   },
   "source": [
    "Adicionando palavras chave da linguagem na tabela de simbolos\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aJtF1gCt2UcV"
   },
   "outputs": [],
   "source": [
    "# Creating symbol table\n",
    "table = SymTable()\n",
    "\n",
    "# Add keywords into symbol table\n",
    "try:\n",
    "    table.addNewEntry(Token('inicio', 'inicio', '-'))\n",
    "    table.addNewEntry(Token('varinicio', 'varinicio', '-'))\n",
    "    table.addNewEntry(Token('varfim', 'varfim', '-'))\n",
    "    table.addNewEntry(Token('escreva', 'escreva', '-'))\n",
    "    table.addNewEntry(Token('leia', 'leia', '-'))\n",
    "    table.addNewEntry(Token('se', 'se', '-'))\n",
    "    table.addNewEntry(Token('entao', 'entao', '-'))\n",
    "    table.addNewEntry(Token('fimse', 'fimse', '-'))\n",
    "    table.addNewEntry(Token('fim', 'fim', '-'))\n",
    "    table.addNewEntry(Token('inteiro', 'inteiro', '-'))\n",
    "    table.addNewEntry(Token('lit', 'lit', '-'))\n",
    "    table.addNewEntry(Token('real', 'real', '-'))    \n",
    "except Exception as e:\n",
    "    print(\"erro: {}\".format(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexema: inicio Token: inicio Tipo: -\n",
      "Lexema: varinicio Token: varinicio Tipo: -\n",
      "Lexema: varfim Token: varfim Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: leia Token: leia Tipo: -\n",
      "Lexema: se Token: se Tipo: -\n",
      "Lexema: entao Token: entao Tipo: -\n",
      "Lexema: fimse Token: fimse Tipo: -\n",
      "Lexema: fim Token: fim Tipo: -\n",
      "Lexema: inteiro Token: inteiro Tipo: -\n",
      "Lexema: lit Token: lit Tipo: -\n",
      "Lexema: real Token: real Tipo: -\n"
     ]
    }
   ],
   "source": [
    "for key in table.symTable:\n",
    "    print(table.symTable[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela dos estados finais\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_tokens = {\n",
    "    1: 'Num',\n",
    "    2: 'Num',\n",
    "    3: 'Num',\n",
    "    6: 'Num',\n",
    "    8: 'Literal',\n",
    "    9: 'id',\n",
    "    11: 'Comentário',\n",
    "    12: 'EOF',\n",
    "    13: 'OPR',\n",
    "    14: 'OPR',\n",
    "    15: 'OPR',\n",
    "    16: 'OPR',\n",
    "    17: 'RCB',\n",
    "    18: 'OPR',\n",
    "    19: 'OPM',\n",
    "    20: 'AB_P',\n",
    "    21: 'FC_P',\n",
    "    22: 'PT_V'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNsCQ-6iomQK"
   },
   "source": [
    "Definição do autômato finito\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicao da class DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4bZOZet-Dqv6"
   },
   "outputs": [],
   "source": [
    "class DFA:\n",
    "    current_state = None;\n",
    "    def __init__(self, states, alphabet, transition_function, start_state, accept_states):\n",
    "        self.states = states;\n",
    "        self.alphabet = alphabet;\n",
    "        self.transition_function = transition_function;\n",
    "        self.start_state = start_state;\n",
    "        self.accept_states = accept_states;\n",
    "        self.current_state = start_state;\n",
    "        return;\n",
    "    \n",
    "    def transition_to_state_with_input(self, input_value):\n",
    "        if((self.current_state, input_value) not in self.transition_function.keys()):\n",
    "            if((self.current_state, '@') in self.transition_function.keys()):\n",
    "                self.current_state = self.transition_function[(self.current_state, '@')];\n",
    "                return True;  \n",
    "            return False;\n",
    "        previous_state = self.current_state;\n",
    "        self.current_state = self.transition_function[(self.current_state, input_value)];\n",
    "        # debug do automato\n",
    "        # print(\"({}, {}) -> {}\".format(previous_state, input_value, self.current_state))\n",
    "        return True;\n",
    "            \n",
    "            \n",
    "    def addToSymbolTable(self, lexema, token):\n",
    "        result = table.getEntry(lexema)\n",
    "        if result:\n",
    "            return result\n",
    "        else: \n",
    "            return table.addNewEntry(token)\n",
    "        \n",
    "    \n",
    "    def in_accept_state(self):\n",
    "        return self.current_state in accept_states;\n",
    "    \n",
    "    def go_to_initial_state(self):\n",
    "        self.current_state = self.start_state;\n",
    "        return;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT6Ud-XkomQL"
   },
   "source": [
    "Definindo função de transição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZhuDqr5VomQL"
   },
   "outputs": [],
   "source": [
    "tf = dict();\n",
    "\n",
    "# Begin tf[(0, D)] = 1;\n",
    "tf[(0, '0')] = 1;\n",
    "tf[(0, '1')] = 1;\n",
    "tf[(0, '2')] = 1;\n",
    "tf[(0, '3')] = 1;\n",
    "tf[(0, '4')] = 1;\n",
    "tf[(0, '5')] = 1;\n",
    "tf[(0, '6')] = 1;\n",
    "tf[(0, '7')] = 1;\n",
    "tf[(0, '8')] = 1;\n",
    "tf[(0, '9')] = 1;\n",
    "# End tf[(0, D)] = 1;\n",
    "\n",
    "# Begin tf[(1, D)] = 1;\n",
    "tf[(1, '0')] = 1;\n",
    "tf[(1, '1')] = 1;\n",
    "tf[(1, '2')] = 1;\n",
    "tf[(1, '3')] = 1;\n",
    "tf[(1, '4')] = 1;\n",
    "tf[(1, '5')] = 1;\n",
    "tf[(1, '6')] = 1;\n",
    "tf[(1, '7')] = 1;\n",
    "tf[(1, '8')] = 1;\n",
    "tf[(1, '9')] = 1;\n",
    "# End tf[(1, D)] = 1;\n",
    "\n",
    "#TODO RESOLVER \\. \n",
    "tf[(1, '.')] = 2;\n",
    "tf[(1, 'E')] = 4;\n",
    "tf[(1, 'e')] = 4;\n",
    "\n",
    "# Begin tf[(2, D)] = 3;\n",
    "tf[(2, '0')] = 3;\n",
    "tf[(2, '1')] = 3;\n",
    "tf[(2, '2')] = 3;\n",
    "tf[(2, '3')] = 3;\n",
    "tf[(2, '4')] = 3;\n",
    "tf[(2, '5')] = 3;\n",
    "tf[(2, '6')] = 3;\n",
    "tf[(2, '7')] = 3;\n",
    "tf[(2, '8')] = 3;\n",
    "tf[(2, '9')] = 3;\n",
    "# End tf[(2, D)] = 3;\n",
    "\n",
    "# TODO VERIFICAR COM O PRATES\n",
    "# tf[(2, \"QUALQUER COISA\")] = 23;\n",
    "\n",
    "# Begin tf[(3, D)] = 3;\n",
    "tf[(3, '0')] = 3;\n",
    "tf[(3, '1')] = 3;\n",
    "tf[(3, '2')] = 3;\n",
    "tf[(3, '3')] = 3;\n",
    "tf[(3, '4')] = 3;\n",
    "tf[(3, '5')] = 3;\n",
    "tf[(3, '6')] = 3;\n",
    "tf[(3, '7')] = 3;\n",
    "tf[(3, '8')] = 3;\n",
    "tf[(3, '9')] = 3;\n",
    "# End tf[(3, D)] = 3;\n",
    "\n",
    "tf[(3, 'E')] = 4;\n",
    "tf[(3, 'e')] = 4;\n",
    "tf[(4, '+')] = 5;\n",
    "tf[(4, '-')] = 5;\n",
    "\n",
    "# Begin tf[(4, D)] = 6;\n",
    "tf[(4, '0')] = 6;\n",
    "tf[(4, '1')] = 6;\n",
    "tf[(4, '2')] = 6;\n",
    "tf[(4, '3')] = 6;\n",
    "tf[(4, '4')] = 6;\n",
    "tf[(4, '5')] = 6;\n",
    "tf[(4, '6')] = 6;\n",
    "tf[(4, '7')] = 6;\n",
    "tf[(4, '8')] = 6;\n",
    "tf[(4, '9')] = 6;\n",
    "# End tf[(4, D)] = 6;\n",
    "\n",
    "# TODO VERIFICAR COM O PRATES (ACONTECEU ERRO?)\n",
    "# tf[(4, 'QUALQUER COISA')] = 23;\n",
    "\n",
    "# Begin tf[(5, D)] = 6;\n",
    "tf[(5, '0')] = 6;\n",
    "tf[(5, '1')] = 6;\n",
    "tf[(5, '2')] = 6;\n",
    "tf[(5, '3')] = 6;\n",
    "tf[(5, '4')] = 6;\n",
    "tf[(5, '5')] = 6;\n",
    "tf[(5, '6')] = 6;\n",
    "tf[(5, '7')] = 6;\n",
    "tf[(5, '8')] = 6;\n",
    "tf[(5, '9')] = 6;\n",
    "# End tf[(5, D)] = 6;\n",
    "\n",
    "# TODO VERIFICAR COM O PRATES (ACONTECEU ERRO?)\n",
    "# tf[(5, 'QUALQUER COISA')] = 23;\n",
    "\n",
    "# Begin tf[(6, D)] = 6;\n",
    "tf[(6, '0')] = 6;\n",
    "tf[(6, '1')] = 6;\n",
    "tf[(6, '2')] = 6;\n",
    "tf[(6, '3')] = 6;\n",
    "tf[(6, '4')] = 6;\n",
    "tf[(6, '5')] = 6;\n",
    "tf[(6, '6')] = 6;\n",
    "tf[(6, '7')] = 6;\n",
    "tf[(6, '8')] = 6;\n",
    "tf[(6, '9')] = 6;\n",
    "# End tf[(6, D)] = 6;\n",
    "\n",
    "tf[(0, '\"')] = 7;\n",
    "tf[(7, '@')] = 7; # aqui o @ substitui ., aceita tudo \n",
    "tf[(7, '\"')] = 8;\n",
    "\n",
    "tf[(0, ' ')] = 23;\n",
    "tf[(0, '\\n')] = 23;\n",
    "tf[(0, '\\t')] = 23;\n",
    "\n",
    "# Begin tf[(0, L)] = 9;\n",
    "tf[(0, 'a')] = 9;\n",
    "tf[(0, 'b')] = 9;\n",
    "tf[(0, 'c')] = 9;\n",
    "tf[(0, 'd')] = 9;\n",
    "tf[(0, 'e')] = 9;\n",
    "tf[(0, 'f')] = 9;\n",
    "tf[(0, 'g')] = 9;\n",
    "tf[(0, 'h')] = 9;\n",
    "tf[(0, 'i')] = 9;\n",
    "tf[(0, 'j')] = 9;\n",
    "tf[(0, 'k')] = 9;\n",
    "tf[(0, 'l')] = 9;\n",
    "tf[(0, 'm')] = 9;\n",
    "tf[(0, 'n')] = 9;\n",
    "tf[(0, 'o')] = 9;\n",
    "tf[(0, 'p')] = 9;\n",
    "tf[(0, 'q')] = 9;\n",
    "tf[(0, 'r')] = 9;\n",
    "tf[(0, 's')] = 9;\n",
    "tf[(0, 't')] = 9;\n",
    "tf[(0, 'u')] = 9;\n",
    "tf[(0, 'v')] = 9;\n",
    "tf[(0, 'w')] = 9;\n",
    "tf[(0, 'x')] = 9;\n",
    "tf[(0, 'y')] = 9;\n",
    "tf[(0, 'z')] = 9;\n",
    "tf[(0, 'A')] = 9;\n",
    "tf[(0, 'B')] = 9;\n",
    "tf[(0, 'C')] = 9;\n",
    "tf[(0, 'D')] = 9;\n",
    "tf[(0, 'E')] = 9;\n",
    "tf[(0, 'F')] = 9;\n",
    "tf[(0, 'G')] = 9;\n",
    "tf[(0, 'H')] = 9;\n",
    "tf[(0, 'I')] = 9;\n",
    "tf[(0, 'J')] = 9;\n",
    "tf[(0, 'K')] = 9;\n",
    "tf[(0, 'L')] = 9;\n",
    "tf[(0, 'M')] = 9;\n",
    "tf[(0, 'N')] = 9;\n",
    "tf[(0, 'O')] = 9;\n",
    "tf[(0, 'P')] = 9;\n",
    "tf[(0, 'Q')] = 9;\n",
    "tf[(0, 'R')] = 9;\n",
    "tf[(0, 'S')] = 9;\n",
    "tf[(0, 'T')] = 9;\n",
    "tf[(0, 'U')] = 9;\n",
    "tf[(0, 'V')] = 9;\n",
    "tf[(0, 'W')] = 9;\n",
    "tf[(0, 'X')] = 9;\n",
    "tf[(0, 'Y')] = 9;\n",
    "tf[(0, 'Z')] = 9;\n",
    "# End tf[(0, L)] = 9;\n",
    "\n",
    "# Begin tf[(9, L)] = 9;\n",
    "tf[(9, 'a')] = 9;\n",
    "tf[(9, 'b')] = 9;\n",
    "tf[(9, 'c')] = 9;\n",
    "tf[(9, 'd')] = 9;\n",
    "tf[(9, 'e')] = 9;\n",
    "tf[(9, 'f')] = 9;\n",
    "tf[(9, 'g')] = 9;\n",
    "tf[(9, 'h')] = 9;\n",
    "tf[(9, 'i')] = 9;\n",
    "tf[(9, 'j')] = 9;\n",
    "tf[(9, 'k')] = 9;\n",
    "tf[(9, 'l')] = 9;\n",
    "tf[(9, 'm')] = 9;\n",
    "tf[(9, 'n')] = 9;\n",
    "tf[(9, 'o')] = 9;\n",
    "tf[(9, 'p')] = 9;\n",
    "tf[(9, 'q')] = 9;\n",
    "tf[(9, 'r')] = 9;\n",
    "tf[(9, 's')] = 9;\n",
    "tf[(9, 't')] = 9;\n",
    "tf[(9, 'u')] = 9;\n",
    "tf[(9, 'v')] = 9;\n",
    "tf[(9, 'w')] = 9;\n",
    "tf[(9, 'x')] = 9;\n",
    "tf[(9, 'y')] = 9;\n",
    "tf[(9, 'z')] = 9;\n",
    "tf[(9, 'A')] = 9;\n",
    "tf[(9, 'B')] = 9;\n",
    "tf[(9, 'C')] = 9;\n",
    "tf[(9, 'D')] = 9;\n",
    "tf[(9, 'E')] = 9;\n",
    "tf[(9, 'F')] = 9;\n",
    "tf[(9, 'G')] = 9;\n",
    "tf[(9, 'H')] = 9;\n",
    "tf[(9, 'I')] = 9;\n",
    "tf[(9, 'J')] = 9;\n",
    "tf[(9, 'K')] = 9;\n",
    "tf[(9, 'L')] = 9;\n",
    "tf[(9, 'M')] = 9;\n",
    "tf[(9, 'N')] = 9;\n",
    "tf[(9, 'O')] = 9;\n",
    "tf[(9, 'P')] = 9;\n",
    "tf[(9, 'Q')] = 9;\n",
    "tf[(9, 'R')] = 9;\n",
    "tf[(9, 'S')] = 9;\n",
    "tf[(9, 'T')] = 9;\n",
    "tf[(9, 'U')] = 9;\n",
    "tf[(9, 'V')] = 9;\n",
    "tf[(9, 'W')] = 9;\n",
    "tf[(9, 'X')] = 9;\n",
    "tf[(9, 'Y')] = 9;\n",
    "tf[(9, 'Z')] = 9;\n",
    "# End tf[(9, L)] = 9;\n",
    "\n",
    "\n",
    "\n",
    "# Begin tf[(9, D)] = 9;\n",
    "tf[(9, '0')] = 9;\n",
    "tf[(9, '1')] = 9;\n",
    "tf[(9, '2')] = 9;\n",
    "tf[(9, '3')] = 9;\n",
    "tf[(9, '4')] = 9;\n",
    "tf[(9, '5')] = 9;\n",
    "tf[(9, '6')] = 9;\n",
    "tf[(9, '7')] = 9;\n",
    "tf[(9, '8')] = 9;\n",
    "tf[(9, '9')] = 9;\n",
    "# End tf[(9, D)] = 9;\n",
    "\n",
    "tf[(9, '_')] = 9;\n",
    "\n",
    "\n",
    "tf[(0, '{')] = 10;\n",
    "tf[(10, '@')] = 10; # aqui o @ substitui ., aceita tudo \n",
    "tf[(10, '}')] = 11;\n",
    "\n",
    "\n",
    "tf[(0, '$')] = 12;\n",
    "\n",
    "tf[(0, '>')] = 13;\n",
    "tf[(13, '=')] = 14;\n",
    "\n",
    "tf[(0, '<')] = 15;\n",
    "tf[(15, '=')] = 14;\n",
    "tf[(15, '>')] = 16;\n",
    "tf[(15, '-')] = 17;\n",
    "\n",
    "tf[(0, '=')] = 18;\n",
    "\n",
    "# Begin tf[(0, OP)] = 19;\n",
    "tf[(0, '+')] = 19;\n",
    "tf[(0, '-')] = 19;\n",
    "tf[(0, '*')] = 19;\n",
    "tf[(0, '/')] = 19;\n",
    "# End tf[(0, OP)] = 19;\n",
    "\n",
    "tf[(0, '(')] = 20;\n",
    "tf[(0, ')')] = 21;\n",
    "tf[(0, ';')] = 22;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicao do alfabeto e estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23};\n",
    "alphabet = {\n",
    "    'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','y','x','z', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'X', 'Z',\n",
    "    '0','1','2','3','4','5','6','7','8','9', \n",
    "    '+','-','*','/', \n",
    "    ' ','{','}','\"',';','(',')','.', '>','<','='};\n",
    "start_state = 0;\n",
    "accept_states = {1, 3, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u6FDOygDjs9"
   },
   "source": [
    "Definição do Analisador Léxico\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7ox1q3FWomQQ"
   },
   "outputs": [],
   "source": [
    "class Scanner:\n",
    "    dfa = None\n",
    "    file = None\n",
    "    rowList = None\n",
    "    tokens = []\n",
    "    errors = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    end_of_analisys = False\n",
    "    \n",
    "    def __init__(self, dfa):  \n",
    "        self.dfa = dfa\n",
    "        self.file = open('fonte.alg')\n",
    "        self.rowList = list(self.file)\n",
    "        \n",
    "    def getCursorPos(self):\n",
    "        return [self.i, self.j];\n",
    "        \n",
    "    def restartAnalisys(self):\n",
    "        self.end_of_analisys = False\n",
    "        self.tokens = []\n",
    "        self.errors = []\n",
    "        self.i = 0 \n",
    "        self.j = 0\n",
    "        \n",
    "    # Continue analisys until next token\n",
    "    def getToken(self):\n",
    "        if self.end_of_analisys:\n",
    "            return Token(\"$\", state_tokens[12], '-')\n",
    "        self.dfa.go_to_initial_state();\n",
    "        lexema = \"\"\n",
    "        first = True\n",
    "        while self.i < len(self.rowList):\n",
    "            charList = list(self.rowList[self.i])\n",
    "            # If is first iteration, get J from last stoped analisys\n",
    "            if first:\n",
    "                first = False\n",
    "            else:\n",
    "                # Else set j = 0 to first char on column\n",
    "                self.j = 0\n",
    "            while self.j < len(charList):\n",
    "                \n",
    "                didTransition = self.dfa.transition_to_state_with_input(charList[self.j]);\n",
    "                # If is \\n or \\t or space or comment, ignore\n",
    "                if(self.dfa.current_state is 23 or self.dfa.current_state is 11):\n",
    "                    self.j = self.j + 1\n",
    "                    self.dfa.go_to_initial_state()\n",
    "                    lexema = \"\"\n",
    "                    continue\n",
    "                    \n",
    "                if didTransition:\n",
    "                    lexema = lexema + charList[self.j]\n",
    "                    # Se for o ultimo lexema, caso seja erro, retornar eof, se não retornar o token e finalizar analise\n",
    "                    if self.i is len(self.rowList)-1 and self.j is len(charList)-1:\n",
    "                        # If is the last token\n",
    "                        if self.dfa.in_accept_state():\n",
    "                            #If has stopped in an accept state\n",
    "                            token = Token(lexema, state_tokens[self.dfa.current_state], '-')\n",
    "                            # Add to symbolTable if is ID \n",
    "                            if self.dfa.current_state is 9:\n",
    "                                token = self.dfa.addToSymbolTable(lexema, token)\n",
    "                            self.tokens.append(token)\n",
    "                            self.dfa.go_to_initial_state()\n",
    "                            lexema = \"\"\n",
    "                            self.end_of_analisys = True\n",
    "                            return token;\n",
    "                        else:\n",
    "                            self.errors.append([lexema, self.i+1, self.j+1])\n",
    "                            print(\"!!! ERRO: Caracter {} não permitido. Na linha {} e coluna {}.\".format(charList[self.j], self.i+1, self.j+1))\n",
    "                            lexema = \"\"\n",
    "                            self.dfa.go_to_initial_state()\n",
    "                            return Token(\"$\", state_tokens[12], '-')\n",
    "                            \n",
    "                else:\n",
    "                    if self.dfa.in_accept_state():\n",
    "                        token = Token(lexema, state_tokens[self.dfa.current_state], '-')\n",
    "                        # Add to symbolTable if is ID \n",
    "                        if self.dfa.current_state is 9:\n",
    "                            token = self.dfa.addToSymbolTable(lexema, token)\n",
    "                        self.tokens.append(token)\n",
    "                        self.dfa.go_to_initial_state()\n",
    "                        lexema = \"\"\n",
    "                        return token;\n",
    "                    else: \n",
    "                        self.errors.append([lexema + charList[self.j], self.i+1, self.j+1])\n",
    "                        print(\"!!! ERRO: Caracter {} não permitido. Na linha {} e coluna {}.\".format(lexema + charList[self.j], self.i+1, self.j+1))\n",
    "                        lexema = \"\"\n",
    "                        self.dfa.go_to_initial_state()\n",
    "\n",
    "                self.j = self.j + 1\n",
    "            self.i = self.i + 1\n",
    "            \n",
    "            \n",
    "    def doAnalisys(self):\n",
    "        i = 0\n",
    "        j = 0\n",
    "        self.dfa.go_to_initial_state();\n",
    "        tokens = []\n",
    "        lexema = \"\"\n",
    "        \n",
    "        while i < len(self.rowList):\n",
    "            charList = list(self.rowList[i])\n",
    "            j = 0\n",
    "            while j < len(charList):\n",
    "                \n",
    "                didTransition = self.dfa.transition_to_state_with_input(charList[j]);\n",
    "    \n",
    "                if(self.dfa.current_state is 23 or self.dfa.current_state is 11):\n",
    "                    j = j + 1\n",
    "                    self.dfa.go_to_initial_state()\n",
    "                    lexema = \"\"\n",
    "                    continue\n",
    "                    \n",
    "                if didTransition:\n",
    "                    lexema = lexema + charList[j]\n",
    "                    if i is len(self.rowList)-1 and j is len(charList)-1:\n",
    "                        # If is the last token\n",
    "                        if self.dfa.in_accept_state():\n",
    "                            #If has stopped in an accept state\n",
    "                            token = Token(lexema, state_tokens[self.dfa.current_state], '-')\n",
    "                            print(token)\n",
    "                            self.tokens.append(token)\n",
    "                            self.dfa.go_to_initial_state()\n",
    "                            lexema = \"\"\n",
    "                        else:\n",
    "                            self.errors.append([lexema, i+1, j+1])\n",
    "                            print(\"!!! ERRO: Caracter {} não permitido. Na linha {} e coluna {}.\".format(charList[j], i+1, j+1))\n",
    "                            lexema = \"\"\n",
    "                            self.dfa.go_to_initial_state()\n",
    "                            \n",
    "                else:\n",
    "                    if self.dfa.in_accept_state():\n",
    "                        token = Token(lexema, state_tokens[self.dfa.current_state], '-')\n",
    "                        token = self.dfa.addToSymbolTable(lexema, token)\n",
    "                        print(token)\n",
    "                        self.tokens.append(token)\n",
    "                        self.dfa.go_to_initial_state()\n",
    "                        lexema = \"\"\n",
    "                        continue\n",
    "                    else: \n",
    "                        self.errors.append([lexema + charList[j], i+1, j+1])\n",
    "                        print(\"!!! ERRO: Caracter {} não permitido. Na linha {} e coluna {}.\".format(lexema + charList[j], i+1, j+1))\n",
    "                        lexema = \"\"\n",
    "                        self.dfa.go_to_initial_state()\n",
    "\n",
    "                j = j + 1\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DFA(states, alphabet, tf, start_state, accept_states);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner = Scanner(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner.restartAnalisys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexema: inicio Token: inicio Tipo: -\n",
      "Lexema: varinicio Token: varinicio Tipo: -\n",
      "Lexema: A Token: id Tipo: -\n",
      "Lexema: lit Token: lit Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: inteiro Token: inteiro Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: D Token: id Tipo: -\n",
      "Lexema: inteiro Token: inteiro Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: C Token: id Tipo: -\n",
      "Lexema: real Token: real Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: varfim Token: varfim Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"Digite B\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: leia Token: leia Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"Digite A:\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: leia Token: leia Tipo: -\n",
      "Lexema: A Token: id Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: se Token: se Tipo: -\n",
      "Lexema: ( Token: AB_P Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: > Token: OPR Tipo: -\n",
      "Lexema: 2 Token: Num Tipo: -\n",
      "Lexema: ) Token: FC_P Tipo: -\n",
      "Lexema: entao Token: entao Tipo: -\n",
      "Lexema: A Token: id Tipo: -\n",
      "Lexema: se Token: se Tipo: -\n",
      "Lexema: ( Token: AB_P Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: <= Token: OPR Tipo: -\n",
      "Lexema: 4 Token: Num Tipo: -\n",
      "Lexema: ) Token: FC_P Tipo: -\n",
      "Lexema: entao Token: entao Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"B esta entre 2 e 4\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: fimse Token: fimse Tipo: -\n",
      "Lexema: fimse Token: fimse Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: + Token: OPM Tipo: -\n",
      "Lexema: 1 Token: Num Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: + Token: OPM Tipo: -\n",
      "Lexema: 2 Token: Num Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: + Token: OPM Tipo: -\n",
      "Lexema: 3 Token: Num Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: D Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: C Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: 5.0 Token: Num Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"\\nB=\\n\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: D Token: id Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"\\n\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: C Token: id Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"\\n\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: A Token: id Tipo: -\n",
      "Lexema: ; Token: PT_V Tipo: -\n",
      "Lexema: fim Token: fim Tipo: -\n",
      "Lexema: $ Token: EOF Tipo: -\n"
     ]
    }
   ],
   "source": [
    "token = scanner.getToken()\n",
    "print(token)\n",
    "while token.token is not \"EOF\":\n",
    "    token = scanner.getToken()\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = scanner.getToken()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EOF'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKENS\n",
      "\n",
      "Foram reconhecidos 93 tokens: \n",
      "\n",
      "LEXEMA                TOKEN      TIPO\n",
      "--------------------  ---------  ------\n",
      "inicio                inicio     -\n",
      "varinicio             varinicio  -\n",
      "A                     id         -\n",
      "lit                   lit        -\n",
      ";                     PT_V       -\n",
      "B                     id         -\n",
      "inteiro               inteiro    -\n",
      ";                     PT_V       -\n",
      "D                     id         -\n",
      "inteiro               inteiro    -\n",
      ";                     PT_V       -\n",
      "C                     id         -\n",
      "real                  real       -\n",
      ";                     PT_V       -\n",
      "varfim                varfim     -\n",
      ";                     PT_V       -\n",
      "escreva               escreva    -\n",
      "\"Digite B\"            Literal    -\n",
      ";                     PT_V       -\n",
      "leia                  leia       -\n",
      "B                     id         -\n",
      ";                     PT_V       -\n",
      "escreva               escreva    -\n",
      "\"Digite A:\"           Literal    -\n",
      ";                     PT_V       -\n",
      "leia                  leia       -\n",
      "A                     id         -\n",
      ";                     PT_V       -\n",
      "se                    se         -\n",
      "(                     AB_P       -\n",
      "B                     id         -\n",
      ">                     OPR        -\n",
      "2                     Num        -\n",
      ")                     FC_P       -\n",
      "entao                 entao      -\n",
      "A                     id         -\n",
      "se                    se         -\n",
      "(                     AB_P       -\n",
      "B                     id         -\n",
      "<=                    OPR        -\n",
      "4                     Num        -\n",
      ")                     FC_P       -\n",
      "entao                 entao      -\n",
      "escreva               escreva    -\n",
      "\"B esta entre 2 e 4\"  Literal    -\n",
      ";                     PT_V       -\n",
      "fimse                 fimse      -\n",
      "fimse                 fimse      -\n",
      "B                     id         -\n",
      "<-                    RCB        -\n",
      "B                     id         -\n",
      "+                     OPM        -\n",
      "1                     Num        -\n",
      ";                     PT_V       -\n",
      "B                     id         -\n",
      "<-                    RCB        -\n",
      "B                     id         -\n",
      "+                     OPM        -\n",
      "2                     Num        -\n",
      ";                     PT_V       -\n",
      "B                     id         -\n",
      "<-                    RCB        -\n",
      "B                     id         -\n",
      "+                     OPM        -\n",
      "3                     Num        -\n",
      ";                     PT_V       -\n",
      "D                     id         -\n",
      "<-                    RCB        -\n",
      "B                     id         -\n",
      ";                     PT_V       -\n",
      "C                     id         -\n",
      "<-                    RCB        -\n",
      "5.0                   Num        -\n",
      ";                     PT_V       -\n",
      "escreva               escreva    -\n",
      "\"\\nB=\\n\"              Literal    -\n",
      ";                     PT_V       -\n",
      "escreva               escreva    -\n",
      "D                     id         -\n",
      ";                     PT_V       -\n",
      "escreva               escreva    -\n",
      "\"\\n\"                  Literal    -\n",
      ";                     PT_V       -\n",
      "escreva               escreva    -\n",
      "C                     id         -\n",
      ";                     PT_V       -\n",
      "escreva               escreva    -\n",
      "\"\\n\"                  Literal    -\n",
      ";                     PT_V       -\n",
      "escreva               escreva    -\n",
      "A                     id         -\n",
      ";                     PT_V       -\n",
      "fim                   fim        -\n",
      "\n",
      "Compilação finalizada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_as_list = []\n",
    "for token in scanner.tokens:\n",
    "    tokens_as_list.append(token.getList())\n",
    "print(\"\\nTOKENS\\n\")\n",
    "print(\"Foram reconhecidos {} tokens: \\n\".format(len(scanner.tokens)))\n",
    "print(tabulate(tokens_as_list, headers=[\"LEXEMA\",\"TOKEN\", \"TIPO\"]))\n",
    "if len(scanner.errors):\n",
    "    print(\"\\nCompilação finalizada com erros:\\n\")\n",
    "    print(\"\\nERROS\\n\")\n",
    "    print(tabulate(scanner.errors, headers=[\"LEXEMA\", \"LINHA\", \"COLUNA\"]))\n",
    "else:\n",
    "    print(\"\\nCompilação finalizada.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisador Sintático:\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gramática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 left: P' length: {'left': \"P'\", 'length': 1}\n",
      "1 left: P length: {'left': 'P', 'length': 3}\n",
      "2 left: V length: {'left': 'V', 'length': 2}\n",
      "3 left: LV length: {'left': 'LV', 'length': 2}\n",
      "4 left: LV length: {'left': 'LV', 'length': 2}\n",
      "5 left: D length: {'left': 'D', 'length': 3}\n",
      "6 left: TIPO length: {'left': 'TIPO', 'length': 1}\n",
      "7 left: TIPO length: {'left': 'TIPO', 'length': 1}\n",
      "8 left: TIPO length: {'left': 'TIPO', 'length': 1}\n",
      "9 left: A length: {'left': 'A', 'length': 2}\n",
      "10 left: ES length: {'left': 'ES', 'length': 3}\n",
      "11 left: ES length: {'left': 'ES', 'length': 3}\n",
      "12 left: ARG length: {'left': 'ARG', 'length': 1}\n",
      "13 left: ARG length: {'left': 'ARG', 'length': 1}\n",
      "14 left: ARG length: {'left': 'ARG', 'length': 1}\n",
      "15 left: A length: {'left': 'A', 'length': 2}\n",
      "16 left: CMD length: {'left': 'CMD', 'length': 4}\n",
      "17 left: LD length: {'left': 'LD', 'length': 3}\n",
      "18 left: LD length: {'left': 'LD', 'length': 1}\n",
      "19 left: OPRD length: {'left': 'OPRD', 'length': 1}\n",
      "20 left: OPRD length: {'left': 'OPRD', 'length': 1}\n",
      "21 left: A length: {'left': 'A', 'length': 2}\n",
      "22 left: COND length: {'left': 'COND', 'length': 2}\n",
      "23 left: CABEÇALHO length: {'left': 'CABEÇALHO', 'length': 5}\n",
      "24 left: EXP_R length: {'left': 'EXP_R', 'length': 3}\n",
      "25 left: CORPO length: {'left': 'CORPO', 'length': 2}\n",
      "26 left: CORPO length: {'left': 'CORPO', 'length': 2}\n",
      "27 left: CORPO length: {'left': 'CORPO', 'length': 2}\n",
      "28 left: CORPO length: {'left': 'CORPO', 'length': 1}\n",
      "29 left: A length: {'left': 'A', 'length': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'Era(m) esperado(s) o(s) token(s) inicio ',\n",
       " 1: None,\n",
       " 2: 'Era(m) esperado(s) o(s) token(s) varinicio ',\n",
       " 3: 'Era(m) esperado(s) o(s) token(s) [fim, leia, escreva,id, se] ',\n",
       " 4: 'Era(m) esperado(s) o(s) token(s) [varfim, id] ',\n",
       " 5: None,\n",
       " 6: None,\n",
       " 7: None,\n",
       " 8: None,\n",
       " 9: None,\n",
       " 10: 'Era(m) esperado(s) o(s) token(s) [id] ',\n",
       " 11: 'Era(m) esperado(s) o(s) token(s) [literal, num, id] ',\n",
       " 12: 'Era(m) esperado(s) o(s) token(s) <- ',\n",
       " 13: 'Era(m) esperado(s) o(s) token(s) fimse ',\n",
       " 14: 'Era(m) esperado(s) o(s) token(s) ( ',\n",
       " 15: None,\n",
       " 16: None,\n",
       " 17: 'Era(m) esperado(s) o(s) token(s) ; ',\n",
       " 18: 'Era(m) esperado(s) o(s) token(s) int,real, lit ',\n",
       " 19: None,\n",
       " 20: None,\n",
       " 21: None,\n",
       " 22: 'Era(m) esperado(s) o(s) token(s) ; ',\n",
       " 23: 'Era(m) esperado(s) o(s) token(s) ; ',\n",
       " 24: None,\n",
       " 25: None,\n",
       " 26: None,\n",
       " 27: None,\n",
       " 28: None,\n",
       " 29: None,\n",
       " 30: None,\n",
       " 31: None,\n",
       " 32: None,\n",
       " 33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: 'Era(m) esperado(s) o(s) token(s) ; ',\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: 'Era(m) esperado(s) o(s) token(s) ; ',\n",
       " 43: 'Era(m) esperado(s) o(s) token(s) + , -, *, /  ',\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 48: None,\n",
       " 49: 'Era(m) esperado(s) o(s) token(s) ) ',\n",
       " 50: 'Era(m) esperado(s) o(s) token(s) <, >, >= , <= , =, <> ',\n",
       " 51: None,\n",
       " 52: None,\n",
       " 53: 'Era(m) esperado(s) o(s) token(s) id, num ',\n",
       " 54: 'Era(m) esperado(s) o(s) token(s) entao ',\n",
       " 55: None,\n",
       " 56: None,\n",
       " 57: None,\n",
       " 58: None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruleRegex = re.compile('(\\D+) (->) (.*)')\n",
    "\n",
    "grammar = dict();\n",
    "\n",
    "rules = [\"P' -> P\",\n",
    "\"P -> inicio V A\",\n",
    "\"V -> varinicio LV\",\n",
    "\"LV -> D LV\",\n",
    "\"LV -> varfim ;\",\n",
    "\"D -> id TIPO ;\",\n",
    "\"TIPO -> int\",\n",
    "\"TIPO -> real\",\n",
    "\"TIPO -> lit\",\n",
    "\"A -> ES A\",\n",
    "\"ES -> leia id ;\",\n",
    "\"ES -> escreva ARG ;\",\n",
    "\"ARG -> literal\",\n",
    "\"ARG -> num\",\n",
    "\"ARG -> id\",\n",
    "\"A -> CMD A\",\n",
    "\"CMD -> id rcb LD ;\",\n",
    "\"LD -> OPRD opm OPRD\",\n",
    "\"LD -> OPRD\",\n",
    "\"OPRD -> id\",\n",
    "\"OPRD -> num\",\n",
    "\"A -> COND A\",\n",
    "\"COND -> CABEÇALHO CORPO\",\n",
    "\"CABEÇALHO -> se ( EXP_R ) então\",\n",
    "\"EXP_R -> OPRD opr OPRD\",\n",
    "\"CORPO -> ES CORPO\",\n",
    "\"CORPO -> CMD CORPO\",\n",
    "\"CORPO -> COND CORPO\",\n",
    "\"CORPO -> fimse\",\n",
    "\"A -> fim\"\n",
    "]\n",
    "\n",
    "\n",
    "numTokenGrammar = [\n",
    "1,\n",
    "3,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "3,\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "2,\n",
    "3,\n",
    "3,\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "2,\n",
    "4,\n",
    "3,\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "2,\n",
    "2,\n",
    "5,\n",
    "3,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "1,\n",
    "1]\n",
    "\n",
    "i = 0\n",
    "for rule in rules:\n",
    "    match = ruleRegex.match(rule)\n",
    "    result = match.group(1)\n",
    "    grammar[i] = dict(left=result, length=numTokenGrammar[i])\n",
    "    print(\"{} left: {} length: {}\".format(i, result, grammar[i]))\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "\n",
    "follow = {}\n",
    "follow[\"P'\"] = ['EOF'] # $ token fim de arquivo\n",
    "follow[\"P\"] = ['EOF'] # $\n",
    "follow[\"V\"] = ['fim', 'leia','escreva','id','se']\n",
    "follow[\"LV\"] = ['fim', 'leia','escreva','id','se']\n",
    "follow[\"D\"] = ['varfim','id']\n",
    "follow[\"TIPO\"] = ['PT_V']\n",
    "follow[\"A\"] = ['EOF'] # $\n",
    "follow[\"ES\"] = ['fim','leia','escreva','id','se','fimse']\n",
    "follow[\"ARG\"] = ['PT_V']\n",
    "follow[\"CMD\"] = ['fim','leia','escreva','id','se','fimse']\n",
    "follow[\"LD\"] = ['PT_V']\n",
    "follow['OPRD'] = ['opm','PT_V','opr','FC_P']\n",
    "follow['COND'] = ['fim','leia','escreva','id','se','fimse']\n",
    "follow['CABEÇALHO'] = ['leia','escreva','id','fimse','se']\n",
    "follow['EXP_R'] = ['FC_P']\n",
    "follow['CORPO'] = ['fim','leia','escreva','id','se','fimse']\n",
    "\n",
    "errors = {}\n",
    "for i in range(59):\n",
    "    errors[i] = None\n",
    "errors[0] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"inicio\")\n",
    "errors[2] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"varinicio\")\n",
    "errors[3] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"[fim, leia, escreva,id, se]\")\n",
    "errors[4] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"[varfim, id]\")\n",
    "errors[10] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"[id]\")\n",
    "errors[11] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"[literal, num, id]\")\n",
    "errors[12] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"<-\")\n",
    "errors[13] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"fimse\")\n",
    "errors[14] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"(\")\n",
    "errors[17] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\";\")\n",
    "errors[18] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"int,real, lit\")\n",
    "errors[22] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\";\")\n",
    "errors[23] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\";\")\n",
    "errors[36] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\";\")\n",
    "errors[42] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\";\")\n",
    "errors[43] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"+ , -, *, / \")\n",
    "errors[49] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\")\")\n",
    "errors[50] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"<, >, >= , <= , =, <>\")\n",
    "errors[53] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"id, num\")\n",
    "errors[54] = \"Era(m) esperado(s) o(s) token(s) {} \".format(\"entao\")\n",
    "errors\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição do analisador sintático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser():\n",
    "    parser_table = {}\n",
    "    scanner = None\n",
    "    stack = []\n",
    "    reduced = []\n",
    "    \n",
    "    #debug apenas\n",
    "    lastReduce = None\n",
    "    lastRuleReduce = None\n",
    "    \n",
    "    def __init__(self, scanner):\n",
    "        self.scanner = scanner\n",
    "        self.scanner.restartAnalisys()\n",
    "        self.loadTable()\n",
    "        self.resetStack()\n",
    "        # Pega sXX ou rXX t.q. XX = id do estado ex: s10 r20  \n",
    "        self.shiftReduceRegex = re.compile('(s|r)(\\d+)')\n",
    "        \n",
    "    def errorReport(self, state, expected, cursor):\n",
    "        if(errors[state]):\n",
    "            print(\"Erro: {} mas foi encontrado {} em linha {} coluna {}\".format(errors[state], expected, cursor[0]+1, cursor[1]))\n",
    "        else:\n",
    "            print(\"Token inesperado: {} em linha {} coluna {}\".format(expected, cursor[0]+1, cursor[1]))\n",
    "        \n",
    "    def errorRecovery(self, symbol, state, a, cursor):\n",
    "#        Descartam-se os símbolos de entrada até encontrar-se um\n",
    "#        elemento de Follow(A), quando também descartamos A\n",
    "#        follow\n",
    "        self.errorReport(state, a.lexema, cursor)\n",
    "        token = self.scanner.getToken()\n",
    "        print(\"debug - erro: \")\n",
    "        while(token.get() is not 'EOF' and self.getAction(token.get(), state)['err']):\n",
    "            print(\"debug - descartando: {}\".format(token.get()))\n",
    "            token = self.scanner.getToken()\n",
    "        print(\"debug - retornando: {}\".format(token.get()))\n",
    "        return token\n",
    "\n",
    "        \n",
    "    def resetStack(self):\n",
    "        self.stack = []\n",
    "        self.stack.append('$')\n",
    "        self.stack.append(0)\n",
    "        \n",
    "    def getTopFromStack(self):\n",
    "#         print(self.stack[len(self.stack) - 1])\n",
    "        top = self.stack[len(self.stack) - 1]\n",
    "        if(isinstance(top, (int, float))):\n",
    "            return int(top)\n",
    "        else:\n",
    "            return top\n",
    "        \n",
    "    def loadTable(self):\n",
    "        self.parser_table = pd.read_csv(\"tabela_sintatica.csv\", delimiter=\",\", names=[\"inicio\",\"varinicio\",\"varfim\",\"PT_V\",\"id\",\"inteiro\",\"real\",\"lit\",\"leia\",\"escreva\",\"Literal\",\"Num\",\"RCB\",\"OPM\",\"se\",\"AB_P\",\"FC_P\",\"entao\",\"OPR\",\"fimse\",\"fim\",\"EOF\",\"P'\",\"P\",\"V\",\"LV\",\"D\",\"TIPO\",\"A\",\"ES\",\"ARG\",\"CMD\",\"LD\",\"OPRD\",\"COND\",\"CABEÇALHO\",\"EXP_R\",\"CORPO\"]) \n",
    "        self.parser_table = self.parser_table.where(pd.notnull(self.parser_table), None)\n",
    "        \n",
    "    def getAction(self, symbol, state):\n",
    "        # inicializa retorno como erro\n",
    "        functionReturn = {}\n",
    "        functionReturn['err'] = True\n",
    "        functionReturn['acc'] = False\n",
    "        functionReturn['action'] = None\n",
    "        functionReturn['state'] = None\n",
    "        \n",
    "        \n",
    "#         print(\"------- debug -------\")\n",
    "#         print(\"symbol: {} state: {}\".format(symbol, state))\n",
    "        \n",
    "        tableResult = self.parser_table[symbol][int(state)]\n",
    "        \n",
    "        # Se a entrada [symbol][state] da tabela não for vazia\n",
    "        # Se for None n entra e retorna erro\n",
    "        if(tableResult):\n",
    "#             print(\"------- debug -------\")\n",
    "#             print(\"tableResult: {}\".format(tableResult))\n",
    "            \n",
    "            # Pega resultado e quebra em aXX t.q. a=acao XX=estado\n",
    "            tableMatch = self.shiftReduceRegex.match(str(tableResult))\n",
    "            \n",
    "#             print(\"tableMatch: {}\".format(tableMatch))\n",
    "            # Se der o match no regex é ou shift ou reduce, então adiciona a acao e o estado, e desativa erro \n",
    "            functionReturn['err'] = False\n",
    "            if(tableMatch):\n",
    "                functionReturn['action'] = tableMatch.group(1)\n",
    "                functionReturn['state'] = int(tableMatch.group(2))\n",
    "            else:\n",
    "                # Se não der match é accept ou um go to\n",
    "                if isinstance(tableResult, (int, float)):\n",
    "                    # Se for numérico é go to\n",
    "                    functionReturn['state'] = int(tableResult)\n",
    "                else:\n",
    "                    functionReturn['acc'] = True                \n",
    "                \n",
    "        return functionReturn\n",
    "    \n",
    "    def parse(self):\n",
    "        self.resetStack()\n",
    "        # pede token pro lexico\n",
    "        a = self.scanner.getToken()\n",
    "        \n",
    "        # Inicializa variaveis\n",
    "        # seja s o estado no topo da pilha\n",
    "        s = None\n",
    "        # B = None\n",
    "        # estado para fazer shift ou estado do goto no reduce\n",
    "        t = None\n",
    "        # lado esquerdo da produção no reduce\n",
    "        A = None\n",
    "        \n",
    "        while True:\n",
    "            # debug imprime topo da pilha\n",
    "            print(self.stack)\n",
    "            # Pega topo da pila\n",
    "            s = self.getTopFromStack()\n",
    "            \n",
    "            # Pega acao\n",
    "            action = self.getAction(a.token, s)\n",
    "            \n",
    "            # Checa acoes\n",
    "            if action['action'] is \"s\":\n",
    "                # shift\n",
    "                t = action['state']\n",
    "                \n",
    "                # empilha t\n",
    "                self.stack.append(t)\n",
    "#                 print(\"shift: token {} s\".format(a.token, s))\n",
    "#                 print(a.token)\n",
    "                \n",
    "                # pega proximo token\n",
    "                a = self.scanner.getToken()\n",
    "\n",
    "            elif action['action'] is \"r\":\n",
    "                # reduce\n",
    "                \n",
    "                # Remove cardinalidade do lado direito da regra da pilha\n",
    "                for item in range(grammar[action['state']]['length']):\n",
    "                    self.stack.pop()\n",
    "#                     print(\"debug: desempilha: {}\".format(self.stack.pop()))\n",
    "                    \n",
    "                # Pega o topo da pilha\n",
    "                t = self.getTopFromStack()\n",
    "                \n",
    "                # Empilha go to\n",
    "                ruleId = action['state']\n",
    "                goto = self.getAction(grammar[ruleId]['left'], t)['state']\n",
    "                self.stack.append(goto)\n",
    "                \n",
    "                # Salva ultimo não terminal DEBUG\n",
    "                self.lastReduce = grammar[ruleId]['left']\n",
    "                self.lastRuleReduce = rules[ruleId]\n",
    "                \n",
    "                # Imprime produção\n",
    "                print(\"reduce: {}\".format(rules[ruleId]))\n",
    "                self.reduced.append(rules[ruleId])\n",
    "                \n",
    "                \n",
    "#                 ruleToReduce = grammar[action['state']]\n",
    "\n",
    "            elif action['acc'] is True:\n",
    "                # accept\n",
    "                print(\"accept\")\n",
    "                break\n",
    "            else: \n",
    "                #reject\n",
    "#                 print(\"Chamando rotina de recuperação de erro sintático ... \")\n",
    "#                 print(\"debug:  error on {} com token {} e regra {}\".format(self.lastReduce, a.get(), self.lastRuleReduce))\n",
    "                a = self.errorRecovery(self.lastReduce, s, a, self.scanner.getCursorPos())\n",
    "                if a.get() is 'EOF':\n",
    "                    break\n",
    "                \n",
    "#                 print(\"error recovery done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"P'\": ['EOF'],\n",
       " 'P': ['EOF'],\n",
       " 'V': ['fim', 'leia', 'escreva', 'id', 'se'],\n",
       " 'LV': ['fim', 'leia', 'escreva', 'id', 'se'],\n",
       " 'D': ['varfim', 'id'],\n",
       " 'TIPO': ['PT_V'],\n",
       " 'A': ['EOF'],\n",
       " 'ES': ['fim', 'leia', 'escreva', 'id', 'se', 'fimse'],\n",
       " 'ARG': ['PT_V'],\n",
       " 'CMD': ['fim', 'leia', 'escreva', 'id', 'se', 'fimse'],\n",
       " 'LD': ['PT_V'],\n",
       " 'OPRD': ['opm', 'PT_V', 'opr', 'FC_P'],\n",
       " 'COND': ['fim', 'leia', 'escreva', 'id', 'se', 'fimse'],\n",
       " 'CABEÇALHO': ['leia', 'escreva', 'id', 'fimse', 'se'],\n",
       " 'EXP_R': ['FC_P'],\n",
       " 'CORPO': ['fim', 'leia', 'escreva', 'id', 'se', 'fimse']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', 0]\n",
      "['$', 0, 2]\n",
      "['$', 0, 2, 4]\n",
      "['$', 0, 2, 4, 18]\n",
      "['$', 0, 2, 4, 18, 39]\n",
      "reduce: TIPO -> lit\n",
      "['$', 0, 2, 4, 18, 36]\n",
      "['$', 0, 2, 4, 18, 36, 51]\n",
      "reduce: D -> id TIPO ;\n",
      "['$', 0, 2, 4, 16]\n",
      "['$', 0, 2, 4, 16, 18]\n",
      "['$', 0, 2, 4, 16, 18, 37]\n",
      "reduce: TIPO -> int\n",
      "['$', 0, 2, 4, 16, 18, 36]\n",
      "['$', 0, 2, 4, 16, 18, 36, 51]\n",
      "reduce: D -> id TIPO ;\n",
      "['$', 0, 2, 4, 16, 16]\n",
      "['$', 0, 2, 4, 16, 16, 18]\n",
      "['$', 0, 2, 4, 16, 16, 18, 37]\n",
      "reduce: TIPO -> int\n",
      "['$', 0, 2, 4, 16, 16, 18, 36]\n",
      "['$', 0, 2, 4, 16, 16, 18, 36, 51]\n",
      "reduce: D -> id TIPO ;\n",
      "['$', 0, 2, 4, 16, 16, 16]\n",
      "['$', 0, 2, 4, 16, 16, 16, 18]\n",
      "['$', 0, 2, 4, 16, 16, 16, 18, 38]\n",
      "reduce: TIPO -> real\n",
      "['$', 0, 2, 4, 16, 16, 16, 18, 36]\n",
      "['$', 0, 2, 4, 16, 16, 16, 18, 36, 51]\n",
      "reduce: D -> id TIPO ;\n",
      "['$', 0, 2, 4, 16, 16, 16, 16]\n",
      "['$', 0, 2, 4, 16, 16, 16, 16, 17]\n",
      "['$', 0, 2, 4, 16, 16, 16, 16, 17, 35]\n",
      "reduce: LV -> varfim ;\n",
      "['$', 0, 2, 4, 16, 16, 16, 16, 34]\n",
      "reduce: LV -> D LV\n",
      "['$', 0, 2, 4, 16, 16, 16, 34]\n",
      "reduce: LV -> D LV\n",
      "['$', 0, 2, 4, 16, 16, 34]\n",
      "reduce: LV -> D LV\n",
      "['$', 0, 2, 4, 16, 34]\n",
      "reduce: LV -> D LV\n",
      "['$', 0, 2, 4, 15]\n",
      "reduce: V -> varinicio LV\n",
      "['$', 0, 2, 3]\n",
      "['$', 0, 2, 3, 11]\n",
      "['$', 0, 2, 3, 11, 24]\n",
      "reduce: ARG -> literal\n",
      "['$', 0, 2, 3, 11, 23]\n",
      "['$', 0, 2, 3, 11, 23, 41]\n",
      "reduce: ES -> escreva ARG ;\n",
      "['$', 0, 2, 3, 6]\n",
      "['$', 0, 2, 3, 6, 10]\n",
      "['$', 0, 2, 3, 6, 10, 22]\n",
      "['$', 0, 2, 3, 6, 10, 22, 40]\n",
      "reduce: ES -> leia id ;\n",
      "['$', 0, 2, 3, 6, 6]\n",
      "['$', 0, 2, 3, 6, 6, 11]\n",
      "['$', 0, 2, 3, 6, 6, 11, 24]\n",
      "reduce: ARG -> literal\n",
      "['$', 0, 2, 3, 6, 6, 11, 23]\n",
      "['$', 0, 2, 3, 6, 6, 11, 23, 41]\n",
      "reduce: ES -> escreva ARG ;\n",
      "['$', 0, 2, 3, 6, 6, 6]\n",
      "['$', 0, 2, 3, 6, 6, 6, 10]\n",
      "['$', 0, 2, 3, 6, 6, 6, 10, 22]\n",
      "['$', 0, 2, 3, 6, 6, 6, 10, 22, 40]\n",
      "reduce: ES -> leia id ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14, 33]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14, 33, 44]\n",
      "reduce: OPRD -> id\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14, 33, 50]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14, 33, 50, 55]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14, 33, 50, 55, 45]\n",
      "reduce: OPRD -> num\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14, 33, 50, 55, 58]\n",
      "reduce: EXP_R -> OPRD opr OPRD\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14, 33, 49]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14, 33, 49, 54]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 14, 33, 49, 54, 57]\n",
      "reduce: CABEÇALHO -> se ( EXP_R ) então\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12]\n",
      "Erro: Era(m) esperado(s) o(s) token(s) <-  mas foi encontrado se em linha 14 coluna 4\n",
      "debug - erro: \n",
      "debug - descartando: AB_P\n",
      "debug - descartando: id\n",
      "debug - descartando: OPR\n",
      "debug - descartando: Num\n",
      "debug - descartando: FC_P\n",
      "debug - descartando: entao\n",
      "debug - descartando: escreva\n",
      "debug - descartando: Literal\n",
      "debug - descartando: PT_V\n",
      "debug - descartando: fimse\n",
      "debug - descartando: fimse\n",
      "debug - descartando: id\n",
      "debug - retornando: RCB\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12, 27]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12, 27, 44]\n",
      "reduce: OPRD -> id\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12, 27, 43]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12, 27, 43, 53]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12, 27, 43, 53, 45]\n",
      "reduce: OPRD -> num\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12, 27, 43, 53, 56]\n",
      "reduce: LD -> OPRD opm OPRD\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12, 27, 42]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 12, 27, 42, 52]\n",
      "reduce: CMD -> id rcb LD ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 12]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 12, 27]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 12, 27, 44]\n",
      "reduce: OPRD -> id\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 12, 27, 43]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 12, 27, 43, 53]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 12, 27, 43, 53, 45]\n",
      "reduce: OPRD -> num\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 12, 27, 43, 53, 56]\n",
      "reduce: LD -> OPRD opm OPRD\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 12, 27, 42]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 12, 27, 42, 52]\n",
      "reduce: CMD -> id rcb LD ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 12]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 12, 27]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 12, 27, 44]\n",
      "reduce: OPRD -> id\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 12, 27, 43]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 12, 27, 43, 53]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 12, 27, 43, 53, 45]\n",
      "reduce: OPRD -> num\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 12, 27, 43, 53, 56]\n",
      "reduce: LD -> OPRD opm OPRD\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 12, 27, 42]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 12, 27, 42, 52]\n",
      "reduce: CMD -> id rcb LD ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 12]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 12, 27]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 12, 27, 44]\n",
      "reduce: OPRD -> id\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 12, 27, 43]\n",
      "reduce: LD -> OPRD\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 12, 27, 42]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 12, 27, 42, 52]\n",
      "reduce: CMD -> id rcb LD ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 12]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 12, 27]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 12, 27, 45]\n",
      "reduce: OPRD -> num\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 12, 27, 43]\n",
      "reduce: LD -> OPRD\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 12, 27, 42]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 12, 27, 42, 52]\n",
      "reduce: CMD -> id rcb LD ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 11]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 11, 24]\n",
      "reduce: ARG -> literal\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 11, 23]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 11, 23, 41]\n",
      "reduce: ES -> escreva ARG ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 11]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 11, 26]\n",
      "reduce: ARG -> id\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 11, 23]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 11, 23, 41]\n",
      "reduce: ES -> escreva ARG ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 11]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 11, 24]\n",
      "reduce: ARG -> literal\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 11, 23]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 11, 23, 41]\n",
      "reduce: ES -> escreva ARG ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 11]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 11, 26]\n",
      "reduce: ARG -> id\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 11, 23]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 11, 23, 41]\n",
      "reduce: ES -> escreva ARG ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 11]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 11, 24]\n",
      "reduce: ARG -> literal\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 11, 23]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 11, 23, 41]\n",
      "reduce: ES -> escreva ARG ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 11]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 11, 26]\n",
      "reduce: ARG -> id\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 11, 23]\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 11, 23, 41]\n",
      "reduce: ES -> escreva ARG ;\n",
      "['$', 0, 2, 3, 6, 6, 6, 6, 13, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29]\n",
      "Token inesperado: fim em linha 30 coluna 2\n",
      "debug - erro: \n",
      "debug - retornando: EOF\n"
     ]
    }
   ],
   "source": [
    "# Instancia autômato finito para análise léxica\n",
    "d = DFA(states, alphabet, tf, start_state, accept_states);\n",
    "\n",
    "# Instancia scanner\n",
    "scanner = Scanner(d)\n",
    "\n",
    "# Reinicia analise léxica\n",
    "scanner.restartAnalisys()\n",
    "\n",
    "# Instancia parser\n",
    "parser = Parser(scanner)\n",
    "\n",
    "parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIPO -> lit\n",
      "D -> id TIPO ;\n",
      "TIPO -> int\n",
      "D -> id TIPO ;\n",
      "TIPO -> int\n",
      "D -> id TIPO ;\n",
      "TIPO -> real\n",
      "D -> id TIPO ;\n",
      "LV -> varfim ;\n",
      "LV -> D LV\n",
      "LV -> D LV\n",
      "LV -> D LV\n",
      "LV -> D LV\n",
      "V -> varinicio LV\n",
      "ARG -> literal\n",
      "ES -> escreva ARG ;\n",
      "ES -> leia id ;\n",
      "ARG -> literal\n",
      "ES -> escreva ARG ;\n",
      "ES -> leia id ;\n",
      "OPRD -> id\n",
      "OPRD -> num\n",
      "EXP_R -> OPRD opr OPRD\n",
      "CABEÇALHO -> se ( EXP_R ) então\n",
      "OPRD -> id\n",
      "OPRD -> num\n",
      "LD -> OPRD opm OPRD\n",
      "CMD -> id rcb LD ;\n",
      "OPRD -> id\n",
      "OPRD -> num\n",
      "LD -> OPRD opm OPRD\n",
      "CMD -> id rcb LD ;\n",
      "OPRD -> id\n",
      "OPRD -> num\n",
      "LD -> OPRD opm OPRD\n",
      "CMD -> id rcb LD ;\n",
      "OPRD -> id\n",
      "LD -> OPRD\n",
      "CMD -> id rcb LD ;\n",
      "OPRD -> num\n",
      "LD -> OPRD\n",
      "CMD -> id rcb LD ;\n",
      "ARG -> literal\n",
      "ES -> escreva ARG ;\n",
      "ARG -> id\n",
      "ES -> escreva ARG ;\n",
      "ARG -> literal\n",
      "ES -> escreva ARG ;\n",
      "ARG -> id\n",
      "ES -> escreva ARG ;\n",
      "ARG -> literal\n",
      "ES -> escreva ARG ;\n",
      "ARG -> id\n",
      "ES -> escreva ARG ;\n"
     ]
    }
   ],
   "source": [
    "for rule in parser.reduced:\n",
    "    print(rule)\n",
    "\n",
    "# for rule in parser.reduced[::-1]:\n",
    "#     print(rule)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sOwtiPdOC3mm",
    "nuVejW89C-QD"
   ],
   "include_colab_link": true,
   "name": "Lexico.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
