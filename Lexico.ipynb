{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rodrigozago/compiladores/blob/master/Lexico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh9K9qYJ9t23"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2vT3_Y0DbZR"
   },
   "source": [
    "Analisador léxico\n",
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOwtiPdOC3mm"
   },
   "source": [
    "Definição da tabela de simbolos\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLc-ORg8omQA"
   },
   "source": [
    "Classe que define a tabela de simbolos.\n",
    "\n",
    "symTable: Estrutura do tipo dict para armazenar os tokens:\n",
    "\n",
    "```{ lexema<string> : token<Token> }```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "stf4cugLAO69"
   },
   "outputs": [],
   "source": [
    "class SymTable:\n",
    "    def __init__(self):\n",
    "        self.symTable = dict();\n",
    "    def addNewEntry(self, token):\n",
    "        if(token.lexema not in self.symTable):\n",
    "            self.symTable[token.lexema] = token\n",
    "        return token\n",
    "    def getEntry(self, lexema):\n",
    "        if(lexema in self.symTable):\n",
    "            return self.symTable[lexema]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuVejW89C-QD"
   },
   "source": [
    "Definição da classe Token\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9532H6Ww13I-"
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, lexema, token, tipo):\n",
    "        self.lexema = lexema\n",
    "        self.token = token\n",
    "        self.tipo = tipo\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Lexema: {} Token: {} Tipo: {}\".format(self.lexema, self.token, self.tipo)\n",
    "\n",
    "    def getList(self):\n",
    "        return [self.lexema, self.token, self.tipo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hygyYiRGDLif"
   },
   "source": [
    "Adicionando palavras chave da linguagem na tabela de simbolos\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aJtF1gCt2UcV"
   },
   "outputs": [],
   "source": [
    "# Creating symbol table\n",
    "table = SymTable()\n",
    "\n",
    "# Add keywords into symbol table\n",
    "try:\n",
    "    table.addNewEntry(Token('inicio', 'inicio', '-'))\n",
    "    table.addNewEntry(Token('varinicio', 'varinicio', '-'))\n",
    "    table.addNewEntry(Token('varfim', 'varfim', '-'))\n",
    "    table.addNewEntry(Token('escreva', 'escreva', '-'))\n",
    "    table.addNewEntry(Token('leia', 'leia', '-'))\n",
    "    table.addNewEntry(Token('se', 'se', '-'))\n",
    "    table.addNewEntry(Token('entao', 'entao', '-'))\n",
    "    table.addNewEntry(Token('fimse', 'fimse', '-'))\n",
    "    table.addNewEntry(Token('fim', 'fim', '-'))\n",
    "    table.addNewEntry(Token('inteiro', 'inteiro', '-'))\n",
    "    table.addNewEntry(Token('lit', 'lit', '-'))\n",
    "    table.addNewEntry(Token('real', 'real', '-'))    \n",
    "except Exception as e:\n",
    "    print(\"erro: {}\".format(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3vi1Dc_uB72u",
    "outputId": "6f8c754c-d0d1-468a-9dd6-3a45fe445268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexema: inicio Token: inicio Tipo: -\n"
     ]
    }
   ],
   "source": [
    "print(table.addNewEntry(Token('inicio', 'inicio', '-')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexema: inicio Token: inicio Tipo: -\n"
     ]
    }
   ],
   "source": [
    "print(table.symTable['inicio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inicio': <__main__.Token at 0x7f7008cc92e8>,\n",
       " 'varinicio': <__main__.Token at 0x7f7008cc9390>,\n",
       " 'varfim': <__main__.Token at 0x7f7008cc93c8>,\n",
       " 'escreva': <__main__.Token at 0x7f7008cc9400>,\n",
       " 'leia': <__main__.Token at 0x7f7008cc9438>,\n",
       " 'se': <__main__.Token at 0x7f7008cc9470>,\n",
       " 'entao': <__main__.Token at 0x7f7008cc94a8>,\n",
       " 'fimse': <__main__.Token at 0x7f7008cc94e0>,\n",
       " 'fim': <__main__.Token at 0x7f7008cc9518>,\n",
       " 'inteiro': <__main__.Token at 0x7f7008cc9550>,\n",
       " 'lit': <__main__.Token at 0x7f7008cc9588>,\n",
       " 'real': <__main__.Token at 0x7f7008cc95c0>,\n",
       " 'A': <__main__.Token at 0x7f7008cde6d8>,\n",
       " 'B': <__main__.Token at 0x7f7008cde320>,\n",
       " 'D': <__main__.Token at 0x7f7008cde8d0>,\n",
       " 'C': <__main__.Token at 0x7f7008cde588>,\n",
       " 'n': <__main__.Token at 0x7f7008cc9e10>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.symTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela dos estados finais\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_tokens = {\n",
    "    1: 'Num',\n",
    "    2: 'Num',\n",
    "    3: 'Num',\n",
    "    6: 'Num',\n",
    "    8: 'Literal',\n",
    "    9: 'id',\n",
    "    11: 'Comentário',\n",
    "    12: 'EOF',\n",
    "    13: 'OPR',\n",
    "    14: 'OPR',\n",
    "    15: 'OPR',\n",
    "    16: 'OPR',\n",
    "    17: 'RCB',\n",
    "    18: 'OPR',\n",
    "    19: 'OPM',\n",
    "    20: 'AB_P',\n",
    "    21: 'FC_P',\n",
    "    22: 'PTV'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNsCQ-6iomQK"
   },
   "source": [
    "Definição do autômato finito\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicao da class DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4bZOZet-Dqv6"
   },
   "outputs": [],
   "source": [
    "class DFA:\n",
    "    current_state = None;\n",
    "    def __init__(self, states, alphabet, transition_function, start_state, accept_states):\n",
    "        self.states = states;\n",
    "        self.alphabet = alphabet;\n",
    "        self.transition_function = transition_function;\n",
    "        self.start_state = start_state;\n",
    "        self.accept_states = accept_states;\n",
    "        self.current_state = start_state;\n",
    "        return;\n",
    "    \n",
    "    def transition_to_state_with_input(self, input_value):\n",
    "        if((self.current_state, input_value) not in self.transition_function.keys()):\n",
    "            if((self.current_state, '@') in self.transition_function.keys()):\n",
    "                self.current_state = self.transition_function[(self.current_state, '@')];\n",
    "                return True;  \n",
    "            return False;\n",
    "        previous_state = self.current_state;\n",
    "        self.current_state = self.transition_function[(self.current_state, input_value)];\n",
    "        # debug do automato\n",
    "        # print(\"({}, {}) -> {}\".format(previous_state, input_value, self.current_state))\n",
    "        return True;\n",
    "            \n",
    "            \n",
    "    def addToSymbolTable(self, lexema, token):\n",
    "        result = table.getEntry(lexema)\n",
    "        if result:\n",
    "            return result\n",
    "        else: \n",
    "            return table.addNewEntry(token)\n",
    "        \n",
    "    \n",
    "    def in_accept_state(self):\n",
    "        return self.current_state in accept_states;\n",
    "    \n",
    "    def go_to_initial_state(self):\n",
    "        self.current_state = self.start_state;\n",
    "        return;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT6Ud-XkomQL"
   },
   "source": [
    "Definindo função de transição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZhuDqr5VomQL"
   },
   "outputs": [],
   "source": [
    "tf = dict();\n",
    "\n",
    "# Begin tf[(0, D)] = 1;\n",
    "tf[(0, '0')] = 1;\n",
    "tf[(0, '1')] = 1;\n",
    "tf[(0, '2')] = 1;\n",
    "tf[(0, '3')] = 1;\n",
    "tf[(0, '4')] = 1;\n",
    "tf[(0, '5')] = 1;\n",
    "tf[(0, '6')] = 1;\n",
    "tf[(0, '7')] = 1;\n",
    "tf[(0, '8')] = 1;\n",
    "tf[(0, '9')] = 1;\n",
    "# End tf[(0, D)] = 1;\n",
    "\n",
    "# Begin tf[(1, D)] = 1;\n",
    "tf[(1, '0')] = 1;\n",
    "tf[(1, '1')] = 1;\n",
    "tf[(1, '2')] = 1;\n",
    "tf[(1, '3')] = 1;\n",
    "tf[(1, '4')] = 1;\n",
    "tf[(1, '5')] = 1;\n",
    "tf[(1, '6')] = 1;\n",
    "tf[(1, '7')] = 1;\n",
    "tf[(1, '8')] = 1;\n",
    "tf[(1, '9')] = 1;\n",
    "# End tf[(1, D)] = 1;\n",
    "\n",
    "#TODO RESOLVER \\. \n",
    "tf[(1, '.')] = 2;\n",
    "tf[(1, 'E')] = 4;\n",
    "tf[(1, 'e')] = 4;\n",
    "\n",
    "# Begin tf[(2, D)] = 3;\n",
    "tf[(2, '0')] = 3;\n",
    "tf[(2, '1')] = 3;\n",
    "tf[(2, '2')] = 3;\n",
    "tf[(2, '3')] = 3;\n",
    "tf[(2, '4')] = 3;\n",
    "tf[(2, '5')] = 3;\n",
    "tf[(2, '6')] = 3;\n",
    "tf[(2, '7')] = 3;\n",
    "tf[(2, '8')] = 3;\n",
    "tf[(2, '9')] = 3;\n",
    "# End tf[(2, D)] = 3;\n",
    "\n",
    "# TODO VERIFICAR COM O PRATES\n",
    "# tf[(2, \"QUALQUER COISA\")] = 23;\n",
    "\n",
    "# Begin tf[(3, D)] = 3;\n",
    "tf[(3, '0')] = 3;\n",
    "tf[(3, '1')] = 3;\n",
    "tf[(3, '2')] = 3;\n",
    "tf[(3, '3')] = 3;\n",
    "tf[(3, '4')] = 3;\n",
    "tf[(3, '5')] = 3;\n",
    "tf[(3, '6')] = 3;\n",
    "tf[(3, '7')] = 3;\n",
    "tf[(3, '8')] = 3;\n",
    "tf[(3, '9')] = 3;\n",
    "# End tf[(3, D)] = 3;\n",
    "\n",
    "tf[(3, 'E')] = 4;\n",
    "tf[(3, 'e')] = 4;\n",
    "tf[(4, '+')] = 5;\n",
    "tf[(4, '-')] = 5;\n",
    "\n",
    "# Begin tf[(4, D)] = 6;\n",
    "tf[(4, '0')] = 6;\n",
    "tf[(4, '1')] = 6;\n",
    "tf[(4, '2')] = 6;\n",
    "tf[(4, '3')] = 6;\n",
    "tf[(4, '4')] = 6;\n",
    "tf[(4, '5')] = 6;\n",
    "tf[(4, '6')] = 6;\n",
    "tf[(4, '7')] = 6;\n",
    "tf[(4, '8')] = 6;\n",
    "tf[(4, '9')] = 6;\n",
    "# End tf[(4, D)] = 6;\n",
    "\n",
    "# TODO VERIFICAR COM O PRATES (ACONTECEU ERRO?)\n",
    "# tf[(4, 'QUALQUER COISA')] = 23;\n",
    "\n",
    "# Begin tf[(5, D)] = 6;\n",
    "tf[(5, '0')] = 6;\n",
    "tf[(5, '1')] = 6;\n",
    "tf[(5, '2')] = 6;\n",
    "tf[(5, '3')] = 6;\n",
    "tf[(5, '4')] = 6;\n",
    "tf[(5, '5')] = 6;\n",
    "tf[(5, '6')] = 6;\n",
    "tf[(5, '7')] = 6;\n",
    "tf[(5, '8')] = 6;\n",
    "tf[(5, '9')] = 6;\n",
    "# End tf[(5, D)] = 6;\n",
    "\n",
    "# TODO VERIFICAR COM O PRATES (ACONTECEU ERRO?)\n",
    "# tf[(5, 'QUALQUER COISA')] = 23;\n",
    "\n",
    "# Begin tf[(6, D)] = 6;\n",
    "tf[(6, '0')] = 6;\n",
    "tf[(6, '1')] = 6;\n",
    "tf[(6, '2')] = 6;\n",
    "tf[(6, '3')] = 6;\n",
    "tf[(6, '4')] = 6;\n",
    "tf[(6, '5')] = 6;\n",
    "tf[(6, '6')] = 6;\n",
    "tf[(6, '7')] = 6;\n",
    "tf[(6, '8')] = 6;\n",
    "tf[(6, '9')] = 6;\n",
    "# End tf[(6, D)] = 6;\n",
    "\n",
    "tf[(0, '\"')] = 7;\n",
    "tf[(7, '@')] = 7; # aqui o @ substitui ., aceita tudo \n",
    "tf[(7, '\"')] = 8;\n",
    "\n",
    "tf[(0, ' ')] = 23;\n",
    "tf[(0, '\\n')] = 23;\n",
    "tf[(0, '\\t')] = 23;\n",
    "\n",
    "# Begin tf[(0, L)] = 9;\n",
    "tf[(0, 'a')] = 9;\n",
    "tf[(0, 'b')] = 9;\n",
    "tf[(0, 'c')] = 9;\n",
    "tf[(0, 'd')] = 9;\n",
    "tf[(0, 'e')] = 9;\n",
    "tf[(0, 'f')] = 9;\n",
    "tf[(0, 'g')] = 9;\n",
    "tf[(0, 'h')] = 9;\n",
    "tf[(0, 'i')] = 9;\n",
    "tf[(0, 'j')] = 9;\n",
    "tf[(0, 'k')] = 9;\n",
    "tf[(0, 'l')] = 9;\n",
    "tf[(0, 'm')] = 9;\n",
    "tf[(0, 'n')] = 9;\n",
    "tf[(0, 'o')] = 9;\n",
    "tf[(0, 'p')] = 9;\n",
    "tf[(0, 'q')] = 9;\n",
    "tf[(0, 'r')] = 9;\n",
    "tf[(0, 's')] = 9;\n",
    "tf[(0, 't')] = 9;\n",
    "tf[(0, 'u')] = 9;\n",
    "tf[(0, 'v')] = 9;\n",
    "tf[(0, 'w')] = 9;\n",
    "tf[(0, 'x')] = 9;\n",
    "tf[(0, 'y')] = 9;\n",
    "tf[(0, 'z')] = 9;\n",
    "tf[(0, 'A')] = 9;\n",
    "tf[(0, 'B')] = 9;\n",
    "tf[(0, 'C')] = 9;\n",
    "tf[(0, 'D')] = 9;\n",
    "tf[(0, 'E')] = 9;\n",
    "tf[(0, 'F')] = 9;\n",
    "tf[(0, 'G')] = 9;\n",
    "tf[(0, 'H')] = 9;\n",
    "tf[(0, 'I')] = 9;\n",
    "tf[(0, 'J')] = 9;\n",
    "tf[(0, 'K')] = 9;\n",
    "tf[(0, 'L')] = 9;\n",
    "tf[(0, 'M')] = 9;\n",
    "tf[(0, 'N')] = 9;\n",
    "tf[(0, 'O')] = 9;\n",
    "tf[(0, 'P')] = 9;\n",
    "tf[(0, 'Q')] = 9;\n",
    "tf[(0, 'R')] = 9;\n",
    "tf[(0, 'S')] = 9;\n",
    "tf[(0, 'T')] = 9;\n",
    "tf[(0, 'U')] = 9;\n",
    "tf[(0, 'V')] = 9;\n",
    "tf[(0, 'W')] = 9;\n",
    "tf[(0, 'X')] = 9;\n",
    "tf[(0, 'Y')] = 9;\n",
    "tf[(0, 'Z')] = 9;\n",
    "# End tf[(0, L)] = 9;\n",
    "\n",
    "# Begin tf[(9, L)] = 9;\n",
    "tf[(9, 'a')] = 9;\n",
    "tf[(9, 'b')] = 9;\n",
    "tf[(9, 'c')] = 9;\n",
    "tf[(9, 'd')] = 9;\n",
    "tf[(9, 'e')] = 9;\n",
    "tf[(9, 'f')] = 9;\n",
    "tf[(9, 'g')] = 9;\n",
    "tf[(9, 'h')] = 9;\n",
    "tf[(9, 'i')] = 9;\n",
    "tf[(9, 'j')] = 9;\n",
    "tf[(9, 'k')] = 9;\n",
    "tf[(9, 'l')] = 9;\n",
    "tf[(9, 'm')] = 9;\n",
    "tf[(9, 'n')] = 9;\n",
    "tf[(9, 'o')] = 9;\n",
    "tf[(9, 'p')] = 9;\n",
    "tf[(9, 'q')] = 9;\n",
    "tf[(9, 'r')] = 9;\n",
    "tf[(9, 's')] = 9;\n",
    "tf[(9, 't')] = 9;\n",
    "tf[(9, 'u')] = 9;\n",
    "tf[(9, 'v')] = 9;\n",
    "tf[(9, 'w')] = 9;\n",
    "tf[(9, 'x')] = 9;\n",
    "tf[(9, 'y')] = 9;\n",
    "tf[(9, 'z')] = 9;\n",
    "tf[(9, 'A')] = 9;\n",
    "tf[(9, 'B')] = 9;\n",
    "tf[(9, 'C')] = 9;\n",
    "tf[(9, 'D')] = 9;\n",
    "tf[(9, 'E')] = 9;\n",
    "tf[(9, 'F')] = 9;\n",
    "tf[(9, 'G')] = 9;\n",
    "tf[(9, 'H')] = 9;\n",
    "tf[(9, 'I')] = 9;\n",
    "tf[(9, 'J')] = 9;\n",
    "tf[(9, 'K')] = 9;\n",
    "tf[(9, 'L')] = 9;\n",
    "tf[(9, 'M')] = 9;\n",
    "tf[(9, 'N')] = 9;\n",
    "tf[(9, 'O')] = 9;\n",
    "tf[(9, 'P')] = 9;\n",
    "tf[(9, 'Q')] = 9;\n",
    "tf[(9, 'R')] = 9;\n",
    "tf[(9, 'S')] = 9;\n",
    "tf[(9, 'T')] = 9;\n",
    "tf[(9, 'U')] = 9;\n",
    "tf[(9, 'V')] = 9;\n",
    "tf[(9, 'W')] = 9;\n",
    "tf[(9, 'X')] = 9;\n",
    "tf[(9, 'Y')] = 9;\n",
    "tf[(9, 'Z')] = 9;\n",
    "# End tf[(9, L)] = 9;\n",
    "\n",
    "\n",
    "\n",
    "# Begin tf[(9, D)] = 9;\n",
    "tf[(9, '0')] = 9;\n",
    "tf[(9, '1')] = 9;\n",
    "tf[(9, '2')] = 9;\n",
    "tf[(9, '3')] = 9;\n",
    "tf[(9, '4')] = 9;\n",
    "tf[(9, '5')] = 9;\n",
    "tf[(9, '6')] = 9;\n",
    "tf[(9, '7')] = 9;\n",
    "tf[(9, '8')] = 9;\n",
    "tf[(9, '9')] = 9;\n",
    "# End tf[(9, D)] = 9;\n",
    "\n",
    "tf[(9, '_')] = 9;\n",
    "\n",
    "\n",
    "tf[(0, '{')] = 10;\n",
    "tf[(10, '@')] = 10; # aqui o @ substitui ., aceita tudo \n",
    "tf[(10, '}')] = 11;\n",
    "\n",
    "\n",
    "tf[(0, '$')] = 12;\n",
    "\n",
    "tf[(0, '>')] = 13;\n",
    "tf[(13, '=')] = 14;\n",
    "\n",
    "tf[(0, '<')] = 15;\n",
    "tf[(15, '=')] = 14;\n",
    "tf[(15, '>')] = 16;\n",
    "tf[(15, '-')] = 17;\n",
    "\n",
    "tf[(0, '=')] = 18;\n",
    "\n",
    "# Begin tf[(0, OP)] = 19;\n",
    "tf[(0, '+')] = 19;\n",
    "tf[(0, '-')] = 19;\n",
    "tf[(0, '*')] = 19;\n",
    "tf[(0, '/')] = 19;\n",
    "# End tf[(0, OP)] = 19;\n",
    "\n",
    "tf[(0, '(')] = 20;\n",
    "tf[(0, ')')] = 21;\n",
    "tf[(0, ';')] = 22;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicao do alfabeto e estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23};\n",
    "alphabet = {\n",
    "    'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','y','x','z', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'X', 'Z',\n",
    "    '0','1','2','3','4','5','6','7','8','9', \n",
    "    '+','-','*','/', \n",
    "    ' ','{','}','\"',';','(',')','.', '>','<','='};\n",
    "start_state = 0;\n",
    "accept_states = {1, 3, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializacao do automato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DFA(states, alphabet, tf, start_state, accept_states);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u6FDOygDjs9"
   },
   "source": [
    "Definição do Analisador Léxico\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7ox1q3FWomQQ"
   },
   "outputs": [],
   "source": [
    "class Scanner:\n",
    "    dfa = None\n",
    "    file = None\n",
    "    rowList = None\n",
    "    tokens = []\n",
    "    errors = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    end_of_analisys = False\n",
    "    \n",
    "    def __init__(self, dfa):  \n",
    "        self.dfa = dfa\n",
    "        self.file = open('fonte.alg')\n",
    "        self.rowList = list(self.file)\n",
    "        \n",
    "    def restartAnalisys(self):\n",
    "        self.end_of_analisys = False\n",
    "        self.tokens = []\n",
    "        self.errors = []\n",
    "        self.i = 0 \n",
    "        self.j = 0\n",
    "        \n",
    "    # Continue analisys until next token\n",
    "    def getToken(self):\n",
    "        if self.end_of_analisys:\n",
    "            return Token(\"$\", state_tokens[12], '-')\n",
    "        self.dfa.go_to_initial_state();\n",
    "        lexema = \"\"\n",
    "        first = True\n",
    "        while self.i < len(self.rowList):\n",
    "            charList = list(self.rowList[self.i])\n",
    "            # If is first iteration, get J from last stoped analisys\n",
    "            if first:\n",
    "                first = False\n",
    "            else:\n",
    "                # Else set j = 0 to first char on column\n",
    "                self.j = 0\n",
    "            while self.j < len(charList):\n",
    "                \n",
    "                didTransition = self.dfa.transition_to_state_with_input(charList[self.j]);\n",
    "                # If is \\n or \\t or space or comment, ignore\n",
    "                if(self.dfa.current_state is 23 or self.dfa.current_state is 11):\n",
    "                    self.j = self.j + 1\n",
    "                    self.dfa.go_to_initial_state()\n",
    "                    lexema = \"\"\n",
    "                    continue\n",
    "                    \n",
    "                if didTransition:\n",
    "                    lexema = lexema + charList[self.j]\n",
    "                    # Se for o ultimo lexema, caso seja erro, retornar eof, se não retornar o token e finalizar analise\n",
    "                    if self.i is len(self.rowList)-1 and self.j is len(charList)-1:\n",
    "                        # If is the last token\n",
    "                        if self.dfa.in_accept_state():\n",
    "                            #If has stopped in an accept state\n",
    "                            token = Token(lexema, state_tokens[self.dfa.current_state], '-')\n",
    "                            self.tokens.append(token)\n",
    "                            self.dfa.go_to_initial_state()\n",
    "                            lexema = \"\"\n",
    "                            end_of_analisys = True\n",
    "                            return token;\n",
    "                        else:\n",
    "                            self.errors.append([lexema, self.i+1, self.j+1])\n",
    "                            print(\"!!! ERRO: Caracter {} não permitido. Na linha {} e coluna {}.\".format(charList[self.j], self.i+1, self.j+1))\n",
    "                            lexema = \"\"\n",
    "                            self.dfa.go_to_initial_state()\n",
    "                            return Token(\"$\", state_tokens[12], '-')\n",
    "                            \n",
    "                else:\n",
    "                    if self.dfa.in_accept_state():\n",
    "                        token = Token(lexema, state_tokens[self.dfa.current_state], '-')\n",
    "                        # Add to symbolTable if is ID \n",
    "                        if self.dfa.current_state is 9:\n",
    "                            token = self.dfa.addToSymbolTable(lexema, token)\n",
    "                        self.tokens.append(token)\n",
    "                        self.dfa.go_to_initial_state()\n",
    "                        lexema = \"\"\n",
    "                        return token;\n",
    "                    else: \n",
    "                        self.errors.append([lexema + charList[self.j], self.i+1, self.j+1])\n",
    "                        print(\"!!! ERRO: Caracter {} não permitido. Na linha {} e coluna {}.\".format(lexema + charList[self.j], self.i+1, self.j+1))\n",
    "                        lexema = \"\"\n",
    "                        self.dfa.go_to_initial_state()\n",
    "\n",
    "                self.j = self.j + 1\n",
    "            self.i = self.i + 1\n",
    "            \n",
    "            \n",
    "    def doAnalisys(self):\n",
    "        i = 0\n",
    "        j = 0\n",
    "        self.dfa.go_to_initial_state();\n",
    "        tokens = []\n",
    "        lexema = \"\"\n",
    "        \n",
    "        while i < len(self.rowList):\n",
    "            charList = list(self.rowList[i])\n",
    "            j = 0\n",
    "            while j < len(charList):\n",
    "                \n",
    "                didTransition = self.dfa.transition_to_state_with_input(charList[j]);\n",
    "    \n",
    "                if(self.dfa.current_state is 23 or self.dfa.current_state is 11):\n",
    "                    j = j + 1\n",
    "                    self.dfa.go_to_initial_state()\n",
    "                    lexema = \"\"\n",
    "                    continue\n",
    "                    \n",
    "                if didTransition:\n",
    "                    lexema = lexema + charList[j]\n",
    "                    if i is len(self.rowList)-1 and j is len(charList)-1:\n",
    "                        # If is the last token\n",
    "                        if self.dfa.in_accept_state():\n",
    "                            #If has stopped in an accept state\n",
    "                            token = Token(lexema, state_tokens[self.dfa.current_state], '-')\n",
    "                            print(token)\n",
    "                            self.tokens.append(token)\n",
    "                            self.dfa.go_to_initial_state()\n",
    "                            lexema = \"\"\n",
    "                        else:\n",
    "                            self.errors.append([lexema, i+1, j+1])\n",
    "                            print(\"!!! ERRO: Caracter {} não permitido. Na linha {} e coluna {}.\".format(charList[j], i+1, j+1))\n",
    "                            lexema = \"\"\n",
    "                            self.dfa.go_to_initial_state()\n",
    "                            \n",
    "                else:\n",
    "                    if self.dfa.in_accept_state():\n",
    "                        token = Token(lexema, state_tokens[self.dfa.current_state], '-')\n",
    "                        token = self.dfa.addToSymbolTable(lexema, token)\n",
    "                        print(token)\n",
    "                        self.tokens.append(token)\n",
    "                        self.dfa.go_to_initial_state()\n",
    "                        lexema = \"\"\n",
    "                        continue\n",
    "                    else: \n",
    "                        self.errors.append([lexema + charList[j], i+1, j+1])\n",
    "                        print(\"!!! ERRO: Caracter {} não permitido. Na linha {} e coluna {}.\".format(lexema + charList[j], i+1, j+1))\n",
    "                        lexema = \"\"\n",
    "                        self.dfa.go_to_initial_state()\n",
    "\n",
    "                j = j + 1\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DFA(states, alphabet, tf, start_state, accept_states);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner = Scanner(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner.restartAnalisys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexema: inicio Token: inicio Tipo: -\n",
      "Lexema: varinicio Token: varinicio Tipo: -\n",
      "Lexema: A Token: id Tipo: -\n",
      "Lexema: lit Token: lit Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: inteiro Token: inteiro Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: D Token: id Tipo: -\n",
      "Lexema: inteiro Token: inteiro Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: C Token: id Tipo: -\n",
      "Lexema: real Token: real Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: varfim Token: varfim Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"Digite B\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: leia Token: leia Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"Digite A:\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: leia Token: leia Tipo: -\n",
      "Lexema: A Token: id Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: se Token: se Tipo: -\n",
      "Lexema: ( Token: AB_P Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: > Token: OPR Tipo: -\n",
      "Lexema: 2 Token: Num Tipo: -\n",
      "Lexema: ) Token: FC_P Tipo: -\n",
      "Lexema: entao Token: entao Tipo: -\n",
      "Lexema: se Token: se Tipo: -\n",
      "Lexema: ( Token: AB_P Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: <= Token: OPR Tipo: -\n",
      "Lexema: 4 Token: Num Tipo: -\n",
      "Lexema: ) Token: FC_P Tipo: -\n",
      "Lexema: entao Token: entao Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"B esta entre 2 e 4\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: fimse Token: fimse Tipo: -\n",
      "Lexema: fimse Token: fimse Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: + Token: OPM Tipo: -\n",
      "Lexema: 1 Token: Num Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: + Token: OPM Tipo: -\n",
      "Lexema: 2 Token: Num Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: + Token: OPM Tipo: -\n",
      "Lexema: 3 Token: Num Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: D Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: B Token: id Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: C Token: id Tipo: -\n",
      "Lexema: <- Token: RCB Tipo: -\n",
      "Lexema: 5.0 Token: Num Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"\\nB=\\n\" Token: Literal Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: D Token: id Tipo: -\n",
      "Lexema: ; Token: PTV Tipo: -\n",
      "Lexema: escreva Token: escreva Tipo: -\n",
      "Lexema: \"\\n;\n",
      "\tescreva C;\n",
      "\tescreva \" Token: Literal Tipo: -\n",
      "!!! ERRO: Caracter \\ não permitido. Na linha 28 e coluna 11.\n",
      "Lexema: n Token: id Tipo: -\n",
      "!!! ERRO: Caracter m não permitido. Na linha 30 e coluna 3.\n",
      "Lexema: $ Token: EOF Tipo: -\n"
     ]
    }
   ],
   "source": [
    "token = scanner.getToken()\n",
    "print(token)\n",
    "while token.token is not \"EOF\":\n",
    "    token = scanner.getToken()\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scanner.doAnalisys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_list = []\n",
    "for token in scanner.tokens:\n",
    "    tokens_as_list.append(token.getList())\n",
    "print(\"\\nTOKENS\\n\")\n",
    "print(\"Foram reconhecidos {} tokens: \\n\".format(len(scanner.tokens)))\n",
    "print(tabulate(tokens_as_list, headers=[\"LEXEMA\",\"TOKEN\", \"TIPO\"]))\n",
    "if len(scanner.errors):\n",
    "    print(\"\\nCompilação finalizada com erros:\\n\")\n",
    "    print(\"\\nERROS\\n\")\n",
    "    print(tabulate(scanner.errors, headers=[\"LEXEMA\", \"LINHA\", \"COLUNA\"]))\n",
    "else:\n",
    "    print(\"\\nCompilação finalizada.\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sOwtiPdOC3mm",
    "nuVejW89C-QD"
   ],
   "include_colab_link": true,
   "name": "Lexico.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
